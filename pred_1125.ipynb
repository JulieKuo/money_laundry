{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import json, copy, pickle, torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!set PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_raw = pd.read_csv(\"data/inverse/info.csv\")\n",
    "ccba_raw = pd.read_csv(\"data/inverse/ccba.csv\")\n",
    "cdtx_raw = pd.read_csv(\"data/inverse/cdtx.csv\")\n",
    "dp_raw = pd.read_csv(\"data/inverse/dp.csv\")\n",
    "remit_raw = pd.read_csv(\"data/inverse/remit.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "a = 1; b = 1\n",
    "fig, ax = plt.subplots(a, b, figsize = (5, 4))\n",
    "sns.countplot(x = \"sar_flag\", data = info_raw, ax = ax)\n",
    "\n",
    "for p in ax.patches:\n",
    "    if np.isnan(p.get_height()):\n",
    "        continue\n",
    "    ax.annotate(f'\\n{int(p.get_height())}', (p.get_x()+0.325, p.get_height()+100), size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alert_key</th>\n",
       "      <th>date</th>\n",
       "      <th>sar_flag</th>\n",
       "      <th>cust_id</th>\n",
       "      <th>risk_rank</th>\n",
       "      <th>occupation_code</th>\n",
       "      <th>total_asset</th>\n",
       "      <th>AGE</th>\n",
       "      <th>lupay</th>\n",
       "      <th>cycam</th>\n",
       "      <th>usgam</th>\n",
       "      <th>clamt</th>\n",
       "      <th>csamt</th>\n",
       "      <th>inamt</th>\n",
       "      <th>cucsm</th>\n",
       "      <th>cucah</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>171142</td>\n",
       "      <td>2021-04-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a39fea9aec90969fe66a2b2b4d1b86368a2d38e8b8d4bf...</td>\n",
       "      <td>3</td>\n",
       "      <td>12.0</td>\n",
       "      <td>241719.0</td>\n",
       "      <td>3</td>\n",
       "      <td>12565.0</td>\n",
       "      <td>150744.0</td>\n",
       "      <td>82748.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12477.0</td>\n",
       "      <td>12477.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>171152</td>\n",
       "      <td>2021-04-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7e42b5dca9b28ee8e5545beb834361e90e6197d176b389...</td>\n",
       "      <td>3</td>\n",
       "      <td>13.0</td>\n",
       "      <td>599497.0</td>\n",
       "      <td>6</td>\n",
       "      <td>3581.0</td>\n",
       "      <td>324783.0</td>\n",
       "      <td>64363.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4981.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>171177</td>\n",
       "      <td>2021-04-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a6cdf6302aead77112013168c6d546d2df3bcb551956d2...</td>\n",
       "      <td>1</td>\n",
       "      <td>19.0</td>\n",
       "      <td>51160.0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>171178</td>\n",
       "      <td>2021-04-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1a3efa69705f611c7ef2384a715c8142e2ee801cfec9df...</td>\n",
       "      <td>3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3634343.0</td>\n",
       "      <td>6</td>\n",
       "      <td>829364.0</td>\n",
       "      <td>7666339.0</td>\n",
       "      <td>2343836.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>781279.0</td>\n",
       "      <td>781279.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>171180</td>\n",
       "      <td>2021-04-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67f8cbb64dd3d447e992b1b299e0ceed3372188e47c88e...</td>\n",
       "      <td>1</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4076287.0</td>\n",
       "      <td>4</td>\n",
       "      <td>636.0</td>\n",
       "      <td>256134.0</td>\n",
       "      <td>3538.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3410.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alert_key        date  sar_flag  \\\n",
       "0     171142  2021-04-01       0.0   \n",
       "1     171152  2021-04-01       0.0   \n",
       "2     171177  2021-04-01       0.0   \n",
       "3     171178  2021-04-01       0.0   \n",
       "4     171180  2021-04-01       0.0   \n",
       "\n",
       "                                             cust_id  risk_rank  \\\n",
       "0  a39fea9aec90969fe66a2b2b4d1b86368a2d38e8b8d4bf...          3   \n",
       "1  7e42b5dca9b28ee8e5545beb834361e90e6197d176b389...          3   \n",
       "2  a6cdf6302aead77112013168c6d546d2df3bcb551956d2...          1   \n",
       "3  1a3efa69705f611c7ef2384a715c8142e2ee801cfec9df...          3   \n",
       "4  67f8cbb64dd3d447e992b1b299e0ceed3372188e47c88e...          1   \n",
       "\n",
       "   occupation_code  total_asset  AGE     lupay      cycam      usgam  clamt  \\\n",
       "0             12.0     241719.0    3   12565.0   150744.0    82748.0    0.0   \n",
       "1             13.0     599497.0    6    3581.0   324783.0    64363.0    0.0   \n",
       "2             19.0      51160.0    4       NaN        NaN        NaN    NaN   \n",
       "3              9.0    3634343.0    6  829364.0  7666339.0  2343836.0    0.0   \n",
       "4             17.0    4076287.0    4     636.0   256134.0     3538.0    0.0   \n",
       "\n",
       "   csamt     inamt     cucsm  cucah  \n",
       "0    0.0   12477.0   12477.0    0.0  \n",
       "1    0.0       0.0    4981.0    0.0  \n",
       "2    NaN       NaN       NaN    NaN  \n",
       "3    0.0  781279.0  781279.0    0.0  \n",
       "4    0.0       0.0    3410.0    0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_raw[\"month\"] = info_raw[\"date\"].apply(lambda X: X[:7])\n",
    "ccba_raw[\"month\"] = ccba_raw[\"byymm\"].apply(lambda X: X[:7])\n",
    "info_raw = pd.merge(info_raw, ccba_raw, on = [\"cust_id\", \"month\"], how = \"left\")\n",
    "info_raw = info_raw.drop([\"month\", \"byymm\"], axis = 1)\n",
    "info_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dp以小時為單位，合併時需以當日最後的時間往前推\n",
    "info_raw[\"date\"] = pd.to_datetime(info_raw[\"date\"] + \" 23\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = [[cdtx_raw, 0, \"date\"], [dp_raw, 1, \"tx_date\"], [remit_raw, 2, \"trans_date\"], [info_raw, 3, \"date\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyclical_feat_encode(df):\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "    df['month'] = df[\"date\"].dt.month\n",
    "    df['day'] = df[\"date\"].dt.day\n",
    "\n",
    "    df['month_sin'] = np.sin(2 * np.pi *  df['month']/ df[\"month\"].max())\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / df[\"month\"].max())\n",
    "\n",
    "    df['day_sin'] = np.sin(2 * np.pi * df['day'] / df[\"day\"].max())\n",
    "    df['day_cos'] = np.cos(2 * np.pi * df['day'] / df[\"day\"].max())\n",
    "\n",
    "    df = df.drop([\"month\", \"day\"], axis = 1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"feats_type.json\", newline='') as file:\n",
    "    feats_type = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_catgorical(df, col):\n",
    "    df[col].fillna('NULL', inplace=True)\n",
    "    map_dict = {v:i for i, v in enumerate(set(df[col].unique()))} #set可排序，NULL放最後\n",
    "    df[col] = df[col].map(map_dict)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_id = {k: i for i, k in enumerate(info_raw[\"cust_id\"].unique())}\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "for i in range(len(data_raw)): # 依序處理各來源資料\n",
    "    df = data_raw[i][0].copy()\n",
    "\n",
    "    df = df.rename(columns = {data_raw[i][2]: \"date\"}) # 統一日期名稱\n",
    "    df = df.drop(df[df[\"date\"].isnull()].index)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"]) #日期type轉換\n",
    "\n",
    "    df = cyclical_feat_encode(df) # 時間特徵生成\n",
    "    \n",
    "    source = data_raw[i][1]\n",
    "    df[\"source\"] = source # 資料源\n",
    "\n",
    "    df[\"cust_id\"] = df[\"cust_id\"].map(cust_id) # label encoding\n",
    "    \n",
    "    # 缺失值處理\n",
    "    for col in df.columns:\n",
    "        if col in [\"source\"]:\n",
    "            continue\n",
    "\n",
    "        if (feats_type[str(source)][col] == \"category\"):\n",
    "            df = process_catgorical(df, col)\n",
    "        elif (feats_type[str(source)][col] in [\"int\", \"float\"]):\n",
    "            df[col].fillna(0, inplace=True)\n",
    "            df[col] = scaler.fit_transform(df[col].to_numpy().reshape(-1, 1))\n",
    "    \n",
    "    data_raw[i][0] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cust_id': 7708,\n",
       " 'country': 128,\n",
       " 'cur_type': 51,\n",
       " 'debit_credit': 2,\n",
       " 'tx_type': 3,\n",
       " 'info_asset_code': 22,\n",
       " 'fiscTxId': 30,\n",
       " 'txbranch': 350,\n",
       " 'cross_bank': 2,\n",
       " 'ATM': 2,\n",
       " 'trans_no': 5,\n",
       " 'sar_flag': 3,\n",
       " 'risk_rank': 4,\n",
       " 'occupation_code': 22,\n",
       " 'AGE': 11}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 各類別的種類數\n",
    "category_num = {}\n",
    "for i in range(4):\n",
    "    for col, v in data_raw[i][0].items():\n",
    "        if col == \"source\":\n",
    "            continue\n",
    "        if (feats_type[str(i)][col] == \"category\"):\n",
    "            category_num[col] = v.nunique()\n",
    "category_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('category_num.json', 'w') as file:\n",
    "    json.dump(category_num, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分群\n",
    "data_g = copy.deepcopy(data_raw)\n",
    "for df_g in data_g:\n",
    "    df_g[0] = df_g[0].groupby(\"cust_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每個id每個info資料所涵蓋的區間之所有資料\n",
    "data = {i:{} for i in cust_id.values()}\n",
    "\n",
    "for id_ in cust_id.values():\n",
    "    # 抓出id的所有資料\n",
    "    cust_data1 = []\n",
    "    for df_g, name, date_col in data_g:\n",
    "        if id_ in df_g.groups:\n",
    "            df1 = df_g.get_group(id_)\n",
    "            cust_data1.extend(df1.to_dict('records'))\n",
    "\n",
    "    # 依日期、資料源排序\n",
    "    cust_data1 = sorted(cust_data1, key = lambda X: (X[\"date\"].timestamp(), X[\"source\"]))\n",
    "    cust_data1 = np.array(cust_data1)\n",
    "\n",
    "    #抓出各區間的資料\n",
    "    cust_data2 = {}\n",
    "    idx = 0\n",
    "    for i, s in enumerate(cust_data1):\n",
    "        if s[\"source\"] == 3:\n",
    "            cust_data2[idx] = {}\n",
    "\n",
    "            if cust_data1[i][\"sar_flag\"] != 2: # 2為missing value\n",
    "                cust_data2[idx][\"sar\"] = cust_data1[i][\"sar_flag\"]\n",
    "                cust_data2[idx][\"data_type\"] = \"train\"\n",
    "            else:\n",
    "                cust_data2[idx][\"data_type\"] = \"test\"\n",
    "\n",
    "            end = i + 1\n",
    "            start = max(0, end - max_len)\n",
    "\n",
    "            cust_data2[idx][\"data\"] = cust_data1[start:end]\n",
    "            idx += 1\n",
    "\n",
    "    data[id_] = cust_data2\n",
    "\n",
    "pickle.dump(data, open('data/inverse/cust_id2.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Process for Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open('data/inverse/cust_id2.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(data, mode = \"train\"):\n",
    "    data_X = []\n",
    "    data_Y = []\n",
    "    for k1, v1 in data.items():\n",
    "        for k2, v2 in v1.items():\n",
    "            # 判斷要抓的是train data還是test data\n",
    "            if mode == \"train\":\n",
    "                if v2[\"data_type\"] == \"test\":\n",
    "                    continue\n",
    "            else:\n",
    "                if v2[\"data_type\"] == \"train\":\n",
    "                    continue\n",
    "            \n",
    "            idx = [[] for _ in range(4)]\n",
    "            data2 = [[] for _ in range(4)]\n",
    "\n",
    "            max_idx = len(v2[\"data\"]) - 1\n",
    "\n",
    "            for i, trade in enumerate(v2[\"data\"]):\n",
    "                source = trade[\"source\"]\n",
    "                trade1 = {k: v for k, v in trade.items() if k not in [\"date\", \"source\", \"alert_key\"]} #刪除日期跟資料來源\n",
    "\n",
    "                if i == max_idx:\n",
    "                    data_Y.append(trade[\"sar_flag\"])\n",
    "                    trade1[\"sar_flag\"] = 2\n",
    "                \n",
    "\n",
    "                idx[source].append(i)\n",
    "                data2[source].append(list(trade1.values()))\n",
    "\n",
    "            data_X.append([idx, data2, trade[\"alert_key\"]])\n",
    "\n",
    "\n",
    "    # train - X shape = (總樣本數(23906), 該樣本下的交易index、內容和alert_key(3), 資料源(4), 數據)\n",
    "    print(f\"Mode: {mode}, Total sample: {len(data_X)}\")\n",
    "\n",
    "    return data_X, data_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: train, Total sample: 23906\n",
      "Mode: test, Total sample: 1845\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = data_process(data, mode = \"train\")\n",
    "X_test, y_test = data_process(data, mode = \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train - 0: 234, 1: 23672, total: 23906, 0/1: 101\n"
     ]
    }
   ],
   "source": [
    "sar1 = sum(y_train)\n",
    "sar0 = len(y_train) - sum(y_train)\n",
    "total = len(y_train)\n",
    "\n",
    "print(f\"train - 0: {sar1}, 1: {sar0}, total: {total}, 0/1: {round(sar0/sar1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: 19124, X_valid: 4782, X_test: 1845\n",
      "y_train: 19124, y_valid: 4782, y_test: 1845\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.2, shuffle = True, stratify = y_train, random_state = 99)\n",
    "print(f\"X_train: {len(X_train)}, X_valid: {len(X_valid)}, X_test: {len(X_test)}\\ny_train: {len(y_train)}, y_valid: {len(y_valid)}, y_test: {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train - 0: 18937, 1: 187, total: 19124\n",
      "valid - 0: 4735, 1: 47, total: 4782\n"
     ]
    }
   ],
   "source": [
    "print(f\"train - 0: {len(y_train) - sum(y_train)}, 1: {sum(y_train)}, total: {len(y_train)}\")\n",
    "print(f\"valid - 0: {len(y_valid) - sum(y_valid)}, 1: {sum(y_valid)}, total: {len(y_valid)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "class Dataset_transform(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.n_samples = len(y)\n",
    "        self.X = X\n",
    "        self.y = torch.tensor(y).float().reshape(-1, 1)\n",
    "                                            \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq_idx = self.X[idx][0]\n",
    "        x1 = self.X[idx][1]\n",
    "        alert_key = self.X[idx][2]\n",
    "        y1 = self.y[idx]\n",
    "\n",
    "\n",
    "        return [torch.tensor(s).long() for s in seq_idx], [torch.tensor(x2) for x2 in x1], y1, alert_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset_transform(X_train, y_train)\n",
    "valid_dataset = Dataset_transform(X_valid, y_valid)\n",
    "test_dataset = Dataset_transform(X_test, y_test)\n",
    "# seq_idx, x1, y1, alert_key = train_dataset.__getitem__(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BatchCollate(data):\n",
    "    batch_idxs = [torch.tensor([], dtype = torch.long) for i in range(4)]\n",
    "    seq_idxs = [torch.tensor([], dtype = torch.long) for i in range(4)]\n",
    "    xs = [torch.tensor([]) for i in range(4)]\n",
    "    targets = torch.tensor([])\n",
    "    alert_keys = []\n",
    "\n",
    "    for batch, d in enumerate(data):\n",
    "        for i in range(4):\n",
    "            seq_idxs[i] = torch.cat((seq_idxs[i], d[0][i]))\n",
    "            xs[i] = torch.cat((xs[i], d[1][i]))\n",
    "            \n",
    "            batch1 = torch.tensor([batch] * len(d[0][i])).long()\n",
    "            batch_idxs[i] = torch.cat((batch_idxs[i], batch1))\n",
    "\n",
    "        targets = torch.cat((targets, d[2]))\n",
    "        alert_keys.append(d[3])\n",
    "\n",
    "\n",
    "    return [batch_idxs, seq_idxs, xs], targets.reshape(-1, 1), alert_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True, collate_fn = BatchCollate)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size = len(valid_dataset), collate_fn = BatchCollate)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = len(test_dataset), collate_fn = BatchCollate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"feats_type.json\", newline='') as file:\n",
    "    feats_type = json.load(file)\n",
    "\n",
    "with open(\"category_num.json\", newline='') as file:\n",
    "    category_num = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    src: https://github.com/baosenguo/Kaggle-MoA-2nd-Place-Solution/blob/main/training/1d-cnn-train.ipynb\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, embed_output=128, hidden_size=512, dropout=0.3):\n",
    "        super().__init__()\n",
    "        cha_1 = 64\n",
    "        cha_2 = 128\n",
    "        cha_3 = 128\n",
    "\n",
    "        cha_1_reshape = int(hidden_size/cha_1)\n",
    "        cha_po_1 = int(hidden_size/cha_1/2)\n",
    "        cha_po_2 = int(hidden_size/cha_1/2/2) * cha_3\n",
    "\n",
    "        self.cha_1 = cha_1\n",
    "        self.cha_2 = cha_2\n",
    "        self.cha_3 = cha_3\n",
    "        self.cha_1_reshape = cha_1_reshape\n",
    "        self.cha_po_1 = cha_po_1\n",
    "        self.cha_po_2 = cha_po_2\n",
    "\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n",
    "\n",
    "        self.batch_norm_c1 = nn.BatchNorm1d(cha_1)\n",
    "        self.dropout_c1 = nn.Dropout(dropout*0.9)\n",
    "        self.conv1 = nn.utils.weight_norm(nn.Conv1d(cha_1,cha_2, kernel_size = 5, stride = 1, padding=2,  bias=False),dim=None)\n",
    "\n",
    "        self.ave_po_c1 = nn.AdaptiveAvgPool1d(output_size = cha_po_1)\n",
    "\n",
    "        self.batch_norm_c2 = nn.BatchNorm1d(cha_2)\n",
    "        self.dropout_c2 = nn.Dropout(dropout*0.8)\n",
    "        self.conv2 = nn.utils.weight_norm(nn.Conv1d(cha_2,cha_2, kernel_size = 3, stride = 1, padding=1, bias=True),dim=None)\n",
    "\n",
    "        self.batch_norm_c2_1 = nn.BatchNorm1d(cha_2)\n",
    "        self.dropout_c2_1 = nn.Dropout(dropout*0.6)\n",
    "        self.conv2_1 = nn.utils.weight_norm(nn.Conv1d(cha_2,cha_2, kernel_size = 3, stride = 1, padding=1, bias=True),dim=None)\n",
    "\n",
    "        self.batch_norm_c2_2 = nn.BatchNorm1d(cha_2)\n",
    "        self.dropout_c2_2 = nn.Dropout(dropout*0.5)\n",
    "        self.conv2_2 = nn.utils.weight_norm(nn.Conv1d(cha_2,cha_3, kernel_size = 5, stride = 1, padding=2, bias=True),dim=None)\n",
    "\n",
    "        self.max_po_c2 = nn.MaxPool1d(kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        self.flt = nn.Flatten()\n",
    "        \n",
    "        self.batch_norm3 = nn.BatchNorm1d(cha_po_2)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(cha_po_2, embed_output))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.celu(self.dense1(x), alpha=0.06)\n",
    "\n",
    "        x = x.reshape(x.shape[0],self.cha_1,\n",
    "                        self.cha_1_reshape)\n",
    "\n",
    "        x = self.batch_norm_c1(x)\n",
    "        x = self.dropout_c1(x)\n",
    "        x = F.relu(self.conv1(x))\n",
    "\n",
    "        x = self.ave_po_c1(x)\n",
    "\n",
    "        x = self.batch_norm_c2(x)\n",
    "        x = self.dropout_c2(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x_s = x\n",
    "\n",
    "        x = self.batch_norm_c2_1(x)\n",
    "        x = self.dropout_c2_1(x)\n",
    "        x = F.relu(self.conv2_1(x))\n",
    "\n",
    "        x = self.batch_norm_c2_2(x)\n",
    "        x = self.dropout_c2_2(x)\n",
    "        x = F.relu(self.conv2_2(x))\n",
    "        x =  x * x_s\n",
    "\n",
    "        x = self.max_po_c2(x)\n",
    "\n",
    "        x = self.flt(x)\n",
    "\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.dense3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from traceback import format_exc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEmbedder(torch.nn.Module):\n",
    "    def __init__(self, feat_type, category_num, embed_dim = 4, embed_output = 32, hidden_size = 256, dropout = 0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        feat_type = {k: v for k, v in feat_type.items() if v in [\"category\", \"int\", \"float\"]}\n",
    "\n",
    "        layers = []\n",
    "        for k, v in feat_type.items():\n",
    "            if v == \"category\":\n",
    "                layers.append(nn.Embedding(category_num[k], embed_dim))\n",
    "            else:\n",
    "                layers.append(nn.Linear(1, embed_dim))\n",
    "        self.embeddings = torch.nn.ModuleList(layers)\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            num_features = len(feat_type) * embed_dim, \n",
    "            embed_output = embed_output, \n",
    "            hidden_size = hidden_size,\n",
    "            dropout = dropout\n",
    "        )\n",
    "\n",
    "        self.feat_type = feat_type\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 透過embedding把每個feature的數據轉成(embed_dim)維的向量\n",
    "        embs = []\n",
    "        \n",
    "        for i, (type_, emb_layer) in enumerate(zip(self.feat_type.values(), self.embeddings)):\n",
    "            if type_ == \"category\": # 類別變數\n",
    "                x1 = emb_layer(x[:, i].long())\n",
    "            else: # 連續變數\n",
    "                x1 = emb_layer(x[:, i].reshape(-1, 1))\n",
    "            \n",
    "            embs.append(x1)\n",
    "        \n",
    "        \n",
    "        embs = torch.cat(embs, dim=1)# 把所有feature的向量合併\n",
    "        embs = self.encoder(embs) # 透過encoder轉換，統一輸出維度(embed_output)\n",
    "            \n",
    "        return embs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 4\n",
    "embed_output = 32\n",
    "embed_hidden_size = 256\n",
    "embed_dropout = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每個資料源建一個embedder\n",
    "layers = []\n",
    "for k, v in feats_type.items():\n",
    "    embedder = FeatureEmbedder(v, category_num, embed_dim, embed_output, embed_hidden_size, embed_dropout)\n",
    "    layers.append(embedder)\n",
    "embedders = torch.nn.ModuleList(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, embedders, input_size, max_len, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedders = embedders\n",
    "        \n",
    "        self.max_len = max_len\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.rnn = nn.LSTM(input_size, hidden_size)\n",
    "        self.norm = nn.BatchNorm1d(hidden_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x, batch):\n",
    "        with torch.no_grad():\n",
    "            # 透過embedder統一各資料源的feature數量\n",
    "            for s in range(4):\n",
    "                if len(x[2][s]) == 1:\n",
    "                    x[2][s] = torch.zeros(1, self.input_size)# 只有1個sample，沒辦法做batch_norm1，encoder會出錯，直接給0\n",
    "                else:\n",
    "                    x[2][s] = self.embedders[s](x[2][s])\n",
    "            \n",
    "\n",
    "            x1 = torch.zeros(batch, self.max_len, self.input_size) # shape: (batch, max_len, features)\n",
    "            \n",
    "            # 合併各資料源的資料\n",
    "            for s in range(4):\n",
    "                for i in range(len(x[0][s])):\n",
    "                    batch_idx, seq_idx, features = x[0][s][i], x[1][s][i], x[2][s][i]\n",
    "                    x1[batch_idx][seq_idx] = features\n",
    "\n",
    "\n",
    "        h0 = torch.randn(1, self.max_len, 16)\n",
    "        c0 = torch.randn(1, self.max_len, 16)\n",
    "\n",
    "\n",
    "        out, (hn, cn) = self.rnn(x1, (h0, c0))\n",
    "        out = self.norm(out[:, -1, :])\n",
    "        out = self.sigmoid(out)\n",
    "        out = self.linear(out)\n",
    "        out = self.sigmoid(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from temporal_aggregator import *\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, embedders, input_size, max_len, hidden_size, temporal_aggregator_type=\"TemporalDebertaAggregator\", \n",
    "                temporal_aggregator_args={\n",
    "                    \"hidden_size\": 32,\n",
    "                    \"num_layers\": 3,\n",
    "                    \"dropout\": 0.3,\n",
    "                    \"max_len\": 512\n",
    "                }\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.embedders = embedders\n",
    "        \n",
    "        self.max_len = max_len\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.temporal_aggregator = eval(\n",
    "            f\"{temporal_aggregator_type}\")(**temporal_aggregator_args)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(temporal_aggregator_args[\"hidden_size\"], 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, batch):\n",
    "        with torch.no_grad():\n",
    "            # 透過embedder統一各資料源的feature數量\n",
    "            for s in range(4):\n",
    "                if len(x[2][s]) == 0:\n",
    "                    continue\n",
    "                elif len(x[2][s]) == 1:\n",
    "                    x[2][s] = torch.zeros(1, self.input_size)# 只有1個sample，沒辦法做batch_norm1，encoder會出錯，直接給0\n",
    "                else:\n",
    "                    x[2][s] = self.embedders[s](x[2][s].to(device))\n",
    "            \n",
    "\n",
    "            x1 = torch.zeros(batch, self.max_len, self.input_size).to(device) # shape: (batch, max_len, features)\n",
    "            mask = torch.zeros((batch, self.max_len)).long().to(device)\n",
    "            \n",
    "            # 合併各資料源的資料\n",
    "            for s in range(4):\n",
    "                for i in range(len(x[0][s])):\n",
    "                    batch_idx, seq_idx, features = x[0][s][i], x[1][s][i], x[2][s][i]\n",
    "                    x1[batch_idx][seq_idx] = features\n",
    "                    mask[batch_idx, seq_idx] = 1\n",
    "\n",
    "\n",
    "        out = self.temporal_aggregator(x1, mask)\n",
    "        out = self.classifier(out).squeeze(-1)\n",
    "\n",
    "        return out.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "model = Model(embedders, embed_output, max_len, hidden_size).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_n(output, target):\n",
    "    comb = list(zip(output, target))\n",
    "    comb.sort(key=lambda x:x[0])\n",
    "    flag = False\n",
    "    for i, (out, gt) in enumerate(comb):\n",
    "        if gt == 1:\n",
    "            if flag:\n",
    "                break\n",
    "            flag = True\n",
    "\n",
    "    recall = ((sum(target)-1) / (len(target)-i))\n",
    "    \n",
    "    return recall.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_weight(labels):\n",
    "    weight = []\n",
    "    for label in labels:\n",
    "        if label == 1:\n",
    "            weight.append(75)\n",
    "        elif label == 0:\n",
    "            weight.append(1)\n",
    "            \n",
    "    return torch.tensor(weight).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate、test預測後的損失函數，以及相關分數\n",
    "def eval_score(dataloader, model, criterion, mode = \"eval\"):\n",
    "    with torch.no_grad():\n",
    "        losses = 0\n",
    "        pred1, y1 = torch.Tensor([]).to(device), torch.Tensor([]).to(device)\n",
    "        for batch, batch_data in enumerate(dataloader):\n",
    "            X, y, alert_key = batch_data\n",
    "            y = y.to(device)\n",
    "\n",
    "            pred = model(X, len(y)) #預測\n",
    "\n",
    "            if mode == \"train\":\n",
    "                weight = loss_weight(y)\n",
    "                criterion = nn.BCELoss(weight = weight).to(device)\n",
    "                loss = criterion(pred, y) #計算損失函數\n",
    "                losses += loss.item()\n",
    "\n",
    "            pred1 = torch.concat([pred1, pred])\n",
    "            y1 = torch.concat([y1, y])\n",
    "\n",
    "\n",
    "        if mode == \"train\":\n",
    "            losses /= (batch + 1)\n",
    "            \n",
    "            recall = recall_n(pred1, y1)\n",
    "\n",
    "            return losses, recall\n",
    "            \n",
    "    return pred1, alert_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#初始化權重，使其符合常態分布\n",
    "for m in model.modules():\n",
    "    if isinstance(m, (nn.Linear)):\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.BatchNorm1d):\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.LSTM):\n",
    "        for name, param in m.named_parameters():\n",
    "            if name.startswith(\"weight\"):\n",
    "                nn.init.kaiming_normal_(param)\n",
    "            else:\n",
    "                nn.init.constant_(param, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 1/100:  23%|██████████████████████████████▍                                                                                                   | 70/299 [00:51<02:48,  1.36it/s, train_loss=0.583]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [28], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m     loss\u001b[39m.\u001b[39mbackward() \u001b[39m# 反向傳播，計算權重對損失函數的梯度\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()  \u001b[39m# 根據梯度更新權重\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m     train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[0;32m     38\u001b[0m \u001b[39mif\u001b[39;00m train_pred \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     39\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 建模\n",
    "best_loss = np.inf\n",
    "paitence = 5\n",
    "remain_patience = 0\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    valid_loss = 0\n",
    "    # train model\n",
    "    model.train() # 模型為訓練模式\n",
    "    \n",
    "    with tqdm(total = len(train_dataloader), ncols = 200) as _tqdm: # 使用需要的參數對tqdm進行初始化\n",
    "        _tqdm.set_description('epoch: {}/{}'.format(epoch + 1, epochs))# 設置前綴 一般為epoch的信息\n",
    "        \n",
    "        for batch, batch_data in enumerate(train_dataloader):\n",
    "            X_train1, y_train1, alert_key = batch_data\n",
    "            y_train1 = y_train1.to(device)\n",
    "\n",
    "            train_pred = model(X_train1, len(y_train1)) #預測\n",
    "\n",
    "            if train_pred == \"error\":\n",
    "                break\n",
    "\n",
    "            weight = loss_weight(y_train1)\n",
    "            criterion = nn.BCELoss(weight = weight).to(device)\n",
    "            loss = criterion(train_pred, y_train1) #計算損失函數\n",
    "            \n",
    "\n",
    "            _tqdm.set_postfix({\"train_loss\" : loss.item()})\n",
    "            _tqdm.update(1)\n",
    "            \n",
    "            optimizer.zero_grad() # 梯度在反向傳播前先清零\n",
    "            loss.backward() # 反向傳播，計算權重對損失函數的梯度\n",
    "            optimizer.step()  # 根據梯度更新權重\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        if train_pred == \"error\":\n",
    "            break\n",
    "            \n",
    "        train_loss /= (batch + 1)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # validate model\n",
    "        model.eval()# 模型為評估模式\n",
    "        valid_loss, valid_recall = eval_score(valid_dataloader, model, criterion, mode = \"train\")\n",
    "        valid_losses.append(valid_loss)\n",
    "\n",
    "       \n",
    "\n",
    "        # 損失函數連續30個epoches都沒下降的話就終止訓練\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            remain_patience = paitence\n",
    "            _tqdm.set_postfix({\"train_loss\" : train_loss, \"valid_loss\": valid_loss, \"best_loss\": best_loss, \"valid_recall\": valid_recall})# 設置想在本次循環監視變量，可作後綴打印出來\n",
    "        else:\n",
    "            _tqdm.set_postfix({\"train_loss\" : train_loss, \"valid_loss\": valid_loss, \"best_loss\": best_loss, \"valid_recall\": valid_recall})# 設置想在本次循環監視變量，可作後綴打印出來\n",
    "            remain_patience -= 1\n",
    "            if remain_patience == 0:\n",
    "                print('early stop!')\n",
    "                break\n",
    "        \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model/20221201.pt\") # 儲存權重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, alert_key = eval_score(test_dataloader, model, criterion, mode = \"eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(zip(alert_key, pred.reshape(-1).tolist()), columns = [\"alert_key\", \"probability\"])\n",
    "result = result.sort_values(\"probability\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv(\"data\\\\submit\\\\預測的案件名單及提交檔案範例.csv\")\n",
    "submit = submit[[\"alert_key\"]]\n",
    "submit = pd.merge(submit, result, on = \"alert_key\", how = \"left\")\n",
    "# submit[\"probability\"] = submit[\"probability\"].astype(float)\n",
    "submit = submit.fillna(1e-6)\n",
    "submit = submit.sort_values(\"probability\", ascending = False)\n",
    "submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv(\"data/submit/1202_1.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a07fcf0145f94b3f971c13d061528107de20ab7b779375f96dab9bbac6a85db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
